== File System ==


-----



__TOC__






-----

=== What is an OS ( I/O Overview ) ===

<blockquote>OS == Operating System 

CPU == Processor

I/O == Input/Output

HD == Hard Drive 

1B == 1 Byte == 8 Bit

KiB == KB == 1024B

MiB == MB == 1024KB 

GiB == GB == 1024MB

Hierarchy [等级制度] 

Concurrency [并行]

Persistence [永久性存留]
</blockquote>
==== Definition ====

* A body of software to make sure the system operates corectly and efficiently on an easy-to use manner, between user APP with Hardware 

==== Goal  ====

<mark>( often contradictory )</mark>

* Convenience for user
* efficiency for machine use/operation

==== Role ( OS == ? )  ====

* Virtural Machine
** extend + simplify interface for physical machine
** provide a library of functions accessible though an API
* Resource Allocator
** allow the resources (HW, SW, data) properly use the system operation
** provide an environment that programs can work uesfully in in this
* Control Program
** ctrl the execution of user programs without error/improper use of PC, especially the programs related to I/O devices

==== OS Themes ====

* Virtualization
** physical resource (CPU, Mem, HDs) → general + powerful + ez-2-use form
** only one/a few really existed resource → multiple/unlimited illusion
* Concurrency
** coordinate multiple activities together to ensure correctness
* Persistence
** Some data needs to survive from crashes or power failures

==== How to relate to other parts of PC system ====

* Covinient abstraction of Hardware
** Address? Page table?
* Protection, Security, Authentication
** PID? 
* Communication
** System call?



=== Storage/Memory ===

<blockquote>HD == Hard Drive

FS == File System

Rudimentary [基本的] == basic

Auxiliary [辅助的] == assistant/ back-up

Volatility [波动性]

Cache [缓存]
</blockquote>
==== Storage/Memory Hierarchy ====

* Rudimentary memory hierarchy
** Processor Register
** Main Memory
** Auxiliary Memory
* Classified by
** Memory Speed
** Cost
** Volatility
* Cache
** install to hide the performance diff between two LVs' large access-time gap
* Larger + Slower + Cheaper
** CPU ( within LV1 Cache) → LV2/LV3 Cache → Main Memory → Disk

==== How to virtualize persistent storage ====

* Convenience <math display="inline">\gets</math> Virtualization + Abstraction
** Let files and directories abstract away from the HD
* Efficiency <math display="inline">\gets</math> Control Program 
** Let FS controls when and how data transferred to persistent storage



=== What is a FS ===

<blockquote>FS == File System

Dominant [支配]

Truncation [截取]
</blockquote>
==== Role ( FS == ? ) ====

* Data Control Program
** Provide long-term information storage, i.e., persistent storage
* File Management System
** Implement an abstraction (i.e. files) for secondary storage
** Organize files logically by directories
** Permit sharing of data between processes, people, and machines
** Protect data from unwanted access (security)

==== Requirement ====

* Store very large amountes of info
* Make info survive from the termination of process
* Make info concurrently accessed by multiple processes

==== File Operations ====

* Creation
** find space in file system
** add entry to directory
** map file name to location and attributes
* Writing
* Reading
** Dominant abstraction: File → Stream
* Reposition within a file
* Deletion
* Truncation/Appending
** May erase all/part of the contents of a file while keeping attributes

==== Handling File Operations ====

* Search d_entry associated with the named file
* When the file is first used, store its attribute info in a System-wide Open-file Table
** Notice: The index into table is used for sub-op, now no searching required



=== File Sharing ===

<blockquote>FS == File System

DB == Database

Synchronization [同步]

Sequential Access [顺序存取]

Semantics [语义]

Identical [一模一样]

Consistency [一致性]

Dangling [悬空]
</blockquote>
==== Role ====

* Basis for communication and synchronization

==== Key Issue ====

* Semantics of concurrent access
** reading process &amp; writign process?
** multi processes open one file for writing?
* Protection

==== Addressing ====

* Using Two Level Internal Tables
*# Per-Process Open File Table (i.e. one for each process)
*#* Contains the current position in each file that certain process has opened
*# System-wide Open File Table (i.e. one for whole system)
*#* Contains the initial attribute info for each file that whole system has opened
* Each entry of 1. has a pointer to a certain entry of 2. for process independent info

==== File Access Method ====

* General-Purpose FS (Simple)
** Sequetial access
*** read fixed bytes one at a time in order
** Direct access
*** random access given block/byte number
* DBFS (Sophisticated)
** Record access
*** fixed/variable length
** Indexed access
* Example in real world
** Unix/Linux
** Windows
* Older == More complicated

==== File Sharing Implementation ====

* File Link 
** a new dir entry with a pointer to another file or subdirectories
* Link Type
** Symbolic/soft link
*** d''entry with pointer to a d''entry that holds true path to the linked file
*** indirect and form a new d''entry point to the d''entry of the linked file
** Hard link
*** d_entry with pointer to the linked file
*** direct and form a new d''entry identical to the d''entry of the linked file
* Issue With Links
** Multiple Absolute Path Per Files
*** should avoid traversing shared structs in FS more than once
** Maintaining Consistency Problem
*** how to update permissions of hard link d_entry
** Shared File Space Deallocation + Reuse
*** deletion of the file entry itself deallocates space and leave the link pointers danling
*** keep a reference count for hard links
** Sharing Signal
*** how to tell when porcesses are sharing the same file



=== Directories ===

<blockquote>Acyclic [非循环]
</blockquote>
==== What are directory and file? ====

* File
** an abstraction to let us refer to persistent data on disk
* Directory
** a logical organization of files
** a special type of file that contains a list of d_entries

==== Role ====

* Logical structure for file systems
** For user
*** a means to organize files
** For FS
*** convenient nameing interface
***# allow the implementation to separate logical file organization from physical file placement
***# store info about files (owner, permission, etc)

==== Structure ====

* Self
** Effectively Random Unorder Entries: ( name, metadata )
** Stored same as files
*** Keep the single kind of secondary storage unit
* Level
** Tree - Structured
*** Single/Two/Multi Level
** Acyclic - Graph
*** Allow for shared directories (same file/subdir may be in 2 different dir)

==== Implementation ====

* Option 1: List
** List &lt; file''name, List&lt; pointer''to''data''block &gt; &gt;
** Linear search to find entries
** Pros
*** Easy to implement
** Cons
*** Slow to execute (notice that directory ops are frequent)
* Option 2: Hash Table
** HashTable &lt;file''name: pointer''to''file''info''struct''in_list &gt;
** Create a list of file info struct
** Pros
*** Fast Time complexity
** Cons
*** Hash Table takes space

=== File System Implementation ===

<blockquote>FS == File System

PC == Personal Computer

Granularity [间隔尺寸]

Replicated [被复制]

Sequential Access [顺序存取]
</blockquote>
==== Structure ====

* FS Block Size
** the standard size defined by FS of each block on the disk
*** disk space is allocated in granularity of blks by this to store files in FS
* Master Block/Partition Control Block/Superblock
** the important structure to connect the disk to PC
** determine the location of root directory
*** always at a well-known disk location
*** often replicated across disk for reliability
*** include other metadata about FS
* Free map
** determine which blocks are free
*** usually a bitmap, one bit per block on the disk
*** also stored on disk, cached in memeory for performance
* Remaining Disk Block/Data Block
** store files and dirs

==== Disk Layout Strategies ====

<ul>
<li><p>Cause</p>
<ul>
<li><p>Files often span multiple disk blocks</p></li></ul>
</li>
<li><p>Major Strategy</p>
<ul>
<li><p>Contiguous Allocation</p>
<ul>
<li><p>all blocks of file are located together on disk</p></li>
<li><p>Pros</p>
<ul>
<li><p>sequential access fast</p></li>
<li><p>allocation fast</p></li>
<li><p>deallocation fast</p></li>
<li><p>small amount of metadata ( i.e. simplfied the directory access )</p></li>
<li><p>allow/ez to indexing</p></li></ul>
</li>
<li><p>Cons</p>
<ul>
<li><p>inflexible</p></li>
<li><p>external fragmentation</p></li>
<li><p>require compaction</p></li>
<li><p>need to move whole files around</p></li></ul>
</li></ul>
</li>
<li><p>Linked/Chained Structure</p>
<ul>
<li><p>each block point to the next, dir pointes to the first</p></li>
<li><p>Pros</p>
<ul>
<li><p>ez sequential/streaming access</p></li>
<li><p>disk blocks can be anywhere</p></li>
<li><p>no external fragmentation</p></li></ul>
</li>
<li><p>Cons</p>
<ul>
<li><p>direct access is expensive</p></li>
<li><p>if one data block is corrupted → lost all rest of file</p></li></ul>
</li></ul>
</li>
<li><p>Indexed Structure </p>
<ul>
<li><p>An index block contains pointers to many other blocks</p></li>
<li><p>May require multiple, linked index blocks</p></li>
<li><p>Pros</p>
<ul>
<li><p>handle reandom accsess well</p></li>
<li><p>small files, still quick for sequential and random access</p></li>
<li><p>no external fragmentation</p></li></ul>
</li>
<li><p>Cons</p>
<ul>
<li><p>limited file size</p></li>
<li><p>↑cost of access bytes near the end of large files </p></li></ul>
</li></ul>
</li></ul>
</li>
<li><p>Unix Inodes</p>
<ul>
<li><p>ITSELF == Indexed Structure ( for files )</p></li>
<li><p>D''entry &lt; file/dir''name, inode''of''file/dir &gt;</p></li>
<li><p>Each indie contains file metadata, + 15 block pointers</p>
<ul>
<li><p>Block [ 0:12 ] == Direct Block Pointer</p>
<ul>
<li><p>disk address of first 12 data blocks of this file/dir</p></li></ul>
</li>
<li><p>Block [ 12 ] == Single Indirect Block Pointer</p>
<ul>
<li><p>disk addr of a block containing the disk addr of data blocks</p>
<ul>
<li><p>contain the disk addr of &gt;12 data blocks of this file/dir</p></li></ul>
</li></ul>
</li>
<li><p>Block [ 13 ] == Double Indirect Block Pointer</p>
<ul>
<li><p>disk addr of a block containing the disk addr of such single indirect blocks </p>
<ul>
<li><p>contain the disk addr of &gt; 12 + 1 <math display="inline">\times</math> BLOCK_SIZE data blocks of this file/dir</p></li></ul>
</li></ul>
</li>
<li><p>Block [ 14 ] == Triple Indirect Block Pointer</p>
<ul>
<li><p>disk addr of a block containing the disk addr of such double indirect blocks </p>
<ul>
<li><p>contain the disk addr of &gt; 12 + 1 <math display="inline">\times</math> BLOCK''SIZE <math display="inline">\times</math> BLOCK''SIZE data blocks of this file/dir</p></li></ul>
</li></ul>
</li></ul>
</li>
<li><p>Imbalenced tree</p>
<ul>
<li><p>Most files are small &lt; 2KB</p></li>
<li><p>Files are usually accessed sequentially</p></li>
<li><p>Dir are typically small &lt; 20 d_entry</p></li></ul>
</li>
<li><p>Example Unix Inode</p>
<ul>
<li><p>Ext2 FS Size == 128 bytes</p></li></ul>
</li></ul>

<p></p></li></ul>

=== FS Design Case: VSFS ===

<blockquote>VSFS == Very Simple File System

OS == Operating System

DS == Data Structure

pointer_size = 4 byte

Disk Volume [磁盘卷]
</blockquote>
==== Main Idea ====

* Create a FS for an Unformatted Disk
** Create some structs in it for data to be ez to find + organize
* Where to store file data + metadata structs (i.e. nodes)
** data blocks + inode blocks
* How to keep track of data allocations
** inode bitmap + data block bitmap
* How to locate file data + metadata
** traverse path
*** superblock <math display="inline">\to</math> root d''entry <math display="inline">\to</math> LV0 root dir inode <math display="inline">\to</math> root dir data blocks <math display="inline">\to</math> d''entries in them <math display="inline">\to</math> LV1 dir/file inodes <math display="inline">\to</math> LV1 dir/file data blocks <math display="inline">\to \cdots</math>
* What are the limitations
** max file size depends on # inode and # hierarchy of inode and block size

==== Overal Orgnization ====

* Input Assumption
** Unformatted Raw Disk Size = <math display="inline">nx</math> KB
** Inode Size = <math display="inline">\frac{x}{1024}</math> KB
** Block Size = <math display="inline">x</math> KB
** #of Inode = <math display="inline">1024m</math>
** max #of block pointer in inode = <math display="inline">p</math> 
*** notice that <math display="inline">4p \leq x</math> since fixed inode size and fixed pointer size
** max #of indirect pointer hierachy in inode = <math display="inline">t</math>
* Conputed Size Value
** #of all blocks = <math display="inline">\frac{\text{Disk Size}}{\text{Block Size}} = n</math>
** #of inode block <math display="inline">\frac{\text{#Inode} \times \text{Inode Size}}{\text{Block Size}} = m</math>
** #of block bitmap = <math display="inline">\frac{\text{#Blocks}}{\text{byte to bit }\times \text{ Block Size}} = \frac{n}{8x}</math> 
** #of inode bitmap = <math display="inline">\frac{\text{#inode}}{\text{byte to bit }\times \text{ Block Size}} = \frac{1024m}{8x}</math>
** #of data blocks = <math display="inline">  n-m-\frac{n}{8x}-\frac{m}{8x}-1</math>
*** #all''blks - #inode''blk - #blk''bitmap - #inode''bitmap - #superblk
** max inode direct pointer = <math display="inline">p - t</math> 
* Limitation
** Max file numer
*** 1024<math display="inline">m</math> (i.e. #of inode Since one file match one inode)
** Max data block use per file
*** <math display="inline">(p-t)+(\frac{1024x}{4})^1+(\frac{1024x}{4})^2+(\frac{1024x}{4})^3+\cdots+(\frac{1024x}{4})^t </math>
*** =#direct data block +<br />
<math display="inline"> (</math> #ptr stored in <math display="inline">LV(t)</math> indirect blk pointing to <math display="inline">LV(t-1)</math> indirect blk <math display="inline">)^t</math>

==== Disk Region ====

* Superblock
** Key Region for mounting FS + Formatting Disk Into FS
*** OS read the superblock FIRST
*** identify FS type and other parameters from this
*** attach the disk volume to FS tree with proper settings
** Structure
*** type of FS (indicated by some magic number)
*** #inodes
*** #data block (not all blks)
*** index of inode table begin
* Inode Table
** Metadata structs storing info of each ( one ) file for FS to track
*** inode''block''id = <math display="inline">\frac{\text{inode id}}{\text{block size}}</math>
** Structure
*** mode
*** file size
*** #block of this file
*** block pointer[ max #of block pointer in inode ]
*** <math display="inline">\cdots</math>
* Allocation Strucure/ Free Map
** keep track of which blks used/free
** Structure 
*** Bitmap (each bit == one block, 0 for free 1 for use)
** Type
**# data block bitmap
**#* #block =<math display="inline">\frac{n}{8x}</math>
**# inode bitmap
**#* #block =<math display="inline">\frac{1024m}{8x}</math>



==== Inodes-Based Approach DS ====

* Inode
** DS representing a FS object like file, dir, link
** attributes indicate metadata file info and disk block location
*** describe where on the disk the blocks for a file/dir are placed
** no file name (file name mapped by another DS d_entry)
** <math display="inline">\ne</math> dir
* Directory Entry
** List &lt;file''name, inode''id&gt; 
*** map file names to inodes
** represent for a file/dir/itself ( . )/parent dir( . )
** To open &quot;/f&quot;
**# use Superblock <math display="inline">\to</math> identify the inode for root dir &quot;/&quot;
**# read root dir inode <math display="inline">\to</math> data block for root dir
**# read data block for root dir <math display="inline">\to</math> look for d_entry of &quot;f&quot;
**# read d_entry <math display="inline">\to</math> identify the inode for &quot;f&quot;
**# read inode of &quot;f&quot; into memory <math display="inline">\to</math> first data block of &quot;f&quot; on disk
**# read first data block of &quot;f&quot; into memory to access the data in the file



==== Other Approaches From Inode to Data ====

* Extent - Based
** A new struct extent &lt; pointer''to''start''block, #continuos''blk &gt;
** Pros
*** use smaller amount of metadata per file
*** file allocation is more compact
** Cons
*** less flexible than the pointer-based approach
*** external fragmentation
** Example
*** EXT4
*** HFS+
*** NTFS
*** XFS
* Link - Based
** LinkList from the first data block to next
** Use an in-memory File Allocation Table indexed by address of data block
** Example
*** FAT



=== Secondary Storage ===

<blockquote>Secondary [次级]

Floppy [软盘]

Volatile [不稳定]
</blockquote>
==== Quality ====

* Large
* Persistent
* Slow

==== Secondary Storage Devices ====

* Drum (ancient story)
* Magnetic Disk
** Fixed
** Removable/Floppy
* Optical Disk
** Write-Once + Read-Many
*** CD-R
*** DVD-R
** Write-Many + Read-Many
*** DVD-RW
* Flash Memory
** Solid State + Non-Volatile Memory



=== Magnetic Disk ===

<blockquote>OS == Operating System

FCFS == First Come First Seek

FIFO == First In First Out

SSTF == Shotest Seek Time First

Actuator [传动装置]

Latency [延迟]

Optimization [最优化]

Skew [时钟相位差]
</blockquote>
==== Disk Physical Structure ====

* metal arm component
** Arm
** Actuator
** Read/Write Head
* circle platter component
** Platter
** Upper Surface
** Lower Surface
* cylinder component
** Track
** Sector ( bar by bar on track)

==== Disk Performance ====

* Performance Dependency
** seeks reduced
** large transfers are used
* Seek
** role
*** move the disk arm to the correct cylinder
** time dependency
*** how fast disk arm can move
** time cost
*** avg 5-6ms depending on distance
** development improving speed
*** very slowly ( 8% per year )
* Rotation
** role
*** waiting for the sector to rotate under the head
** time dependency
*** the rotation rate of disk (7200 RPM SATA)
** time cost
*** avg latency of a half rotation
** development improving speed
*** no change in recent year
* Transfer
** role
*** transferring data from surface into disk controller electronics 
*** sending it back to the host
** time dependency
*** the density (increasing quickly)
** time cost
*** 100MB/S ( avg sector transfer time of 5us )
** development improving speed
*** imporving rapidly ( 40% per year )
* Traditional Service Time Component
** Read ( Transfer) sector1 <math display="inline">\to</math> Seek for sector2 <math display="inline">\to</math> Rotation Latency <math display="inline">\to</math> Read
** OS will minimize particularly seeks and rotation when using the disk

==== Hardware Optimization ====

* Track Skew
** Problem
*** if the arm moves to outer track too slowly <math display="inline">\to</math> miss the close sector
** Address
*** skew the outer track location far at the same time head to meet sector 
* Zone
** Problem
*** each sector has fixed 512 size <math display="inline">\to</math> outer tracks are larger by geometry 
** Address
*** Outer track shoude hold more sectors instead of same #sector /round
* Cache (aka Track Buffer)
** Small Memory Chip
*** part of the hard drive
*** usually 8-16 MB
** Diff VS OS cache
*** it is aware of the disk geometry (OS cache is just an installation)
*** when reading a sector, may cache the whole track to speed up future reads on the same track

==== Disk and the OS ( Disk Interface ) ====

* Disks are messy physical devices
** errors
** bad blocks
** missed seeks
* OS hide the miss in disks from such higher level software
** low-level device control
*** initiate a disk read
** higher-level abstractions
*** file
*** database
* OS may provide diff-level disk access <math display="inline">\to</math> diff-level clients
** physical disk ( access )
*** surface
*** cylinder
*** sector
** logical disk ( access )
*** #block of disk
** logical file ( access )
*** file block
*** record
*** #byte

==== Disk Scheduling ====

<ul>
<li><p>Cause</p>
<ul>
<li><p>seeks are so expensice (avg propotional to ms)</p></li></ul>
</li>
<li><p>Approach</p>
{|
! RequestQueue
! 
! 
! 
! 
! 
|-
| Head @ 53
| 98
| 183
| 37
| 122
| 14
|}

<ul>
<li><p>FCFS </p>
<ul>
<li><p>reasonable when load is low</p></li>
<li><p>long waiting times for log request queues</p></li>
<li><p>example</p>
<ul>
<li><p>scheduling as a simple queue FIFO</p>
<ul>
<li><p>identical with request queue [ 53, 98, 183, 37, 122, 14 ]</p></li></ul>
</li>
<li><p>execution</p>
<ul>
<li><p>53 <math display="inline">\stackrel{+\text{45}}{\longrightarrow}</math> 98 <math display="inline">\stackrel{+85}{\longrightarrow}</math> 183 <math display="inline">\stackrel{-146}{\longrightarrow}</math> 37 <math display="inline">\stackrel{+85}{\longrightarrow}</math> 122 <math display="inline">\stackrel{-108}{\longrightarrow}</math> 14</p></li></ul>
</li>
<li><p>time cost</p>
<ul>
<li><p>|+45| + |+85| + |-146| + |+85| + |-108| = 469</p></li></ul>
</li></ul>
</li></ul>
</li>
<li><p>SSTF</p>
<ul>
<li><p>minimize arm movement ( i.e. seek time )</p></li>
<li><p>maximize request rate</p></li>
<li><p>favor middle blocks</p></li>
<li><p>example</p>
<ul>
<li><p>scheduling as shortest-distance-priority queue</p>
<ul>
<li><p>option 1: for each current head</p>
<ol style="list-style-type: decimal;">
<li><p>sort the current request queue by distance with head</p></li>
<li><p>get the nearest sector</p></li>
<li><p>remove this sector from request queue</p></li>
<li><p>update head by this nearest sector</p></li></ol>
</li>
<li><p>option 2: request queue built as AVL and know current head</p>
<ol style="list-style-type: decimal;">
<li><p>get the next biggest sector of head from the tree</p></li>
<li><p>get the next smallest setor of head from the tree</p></li>
<li><p>select the shortest distance sector from one and two</p></li>
<li><p>remove the current head from request queue AVL</p></li>
<li><p>update current head by selected sector</p></li></ol>
</li>
<li><p>result queue [ 53, 37, 14, 98, 122, 183 ] </p></li></ul>
</li>
<li><p>execution</p>
<ul>
<li><p>53 <math display="inline">\stackrel{-\text{16}}{\longrightarrow}</math> 37 <math display="inline">\stackrel{-23}{\longrightarrow}</math> 14 <math display="inline">\stackrel{+84}{\longrightarrow}</math> 98 <math display="inline">\stackrel{+24}{\longrightarrow}</math> 122 <math display="inline">\stackrel{+61}{\longrightarrow}</math> 183</p></li></ul>
</li>
<li><p>time cost</p>
<ul>
<li><p>|-16| + |-23| + |+84| + |+24| + |+61| = 208</p></li></ul>
</li></ul>
</li></ul>
</li>
<li><p>SCAN/Elevator</p>
<ul>
<li><p>service requests in one direction until sector 0, then reverse</p></li>
<li><p>example</p>
<ul>
<li><p>scheduling as sorted order queue (maybe AVL tree)</p>
<ol style="list-style-type: decimal;">
<li><p>randomly set initial head as pivot sector</p></li>
<li><p>sort the request queue</p></li>
<li><p>divid the sorted request queue as 2 sub queues by pivot</p></li>
<li><p>execute the smaller-than-pivot sub from pivot '''to sector 0'''</p></li>
<li><p>execute the bigger-than-pivot sub queue by insc order</p></li></ol>

<ul>
<li><p>result queue [<u>53</u>, 37, 14, 0, 98, 122, 183]</p></li></ul>
</li>
<li><p>execution</p>
<ul>
<li><p>53 <math display="inline">\stackrel{-\text{16}}{\longrightarrow}</math> 37 <math display="inline">\stackrel{-23}{\longrightarrow}</math> 14 <math display="inline">\stackrel{-14}{\longrightarrow}</math> 0 <math display="inline">\stackrel{+98}{\longrightarrow}</math> 98 <math display="inline">\stackrel{+24}{\longrightarrow}</math> 122 <math display="inline">\stackrel{+61}{\longrightarrow}</math> 183</p></li></ul>
</li>
<li><p>time cost</p>
<ul>
<li><p>|-16| + |-23| + |-14| + |+98| + |+24| + |+61| = 236</p></li></ul>
</li></ul>
</li></ul>
</li>
<li><p>C-SCAN</p>
<ul>
<li><p>like SCAN but only go one direction ( like typewriter )</p></li></ul>
</li>
<li><p>LOOK/C-LOOK</p>
<ul>
<li><p>like SCAN/C-SCAN but only go as far as last request in each direction (not full width of the disk)</p></li>
<li><p>example</p>
<ul>
<li><p>scheduling as sorted order queue (maybe AVL tree)</p>
<ol style="list-style-type: decimal;">
<li><p>randomly set initial head as pivot sector</p></li>
<li><p>sort the request queue</p></li>
<li><p>divid the sorted request queue as 2 sub queues by pivot</p></li>
<li><p>execute the initial head</p></li>
<li><p>execute the smaller-than-pivot sub queue by desc order</p></li>
<li><p>execute the bigger-than-pivot sub queue by insc order</p></li></ol>

<ul>
<li><p>result queue [<u>53</u>, 37, 14, 98, 122, 183]</p></li></ul>
</li>
<li><p>execution</p>
<ul>
<li><p>53 <math display="inline">\stackrel{-\text{16}}{\longrightarrow}</math> 37 <math display="inline">\stackrel{-23}{\longrightarrow}</math> 14 <math display="inline">\stackrel{+84}{\longrightarrow}</math> 98 <math display="inline">\stackrel{+24}{\longrightarrow}</math> 122 <math display="inline">\stackrel{+61}{\longrightarrow}</math> 183</p></li></ul>
</li>
<li><p>time cost</p>
<ul>
<li><p>|-16| + |-23| + |+84| + |+24| + |+61| = 208</p></li></ul>
</li></ul>
</li></ul>
</li></ul>
</li>
<li><p>Impact</p>
<ul>
<li><p>In general, unless there are request queues, disk scheduling low impact</p></li>
<li><p>important for servers but less so for PC</p></li></ul>
</li>
<li><p>Implementation on disks themselves</p>
<ul>
<li><p>disk know their layout better than OS</p></li>
<li><p>disk may ignore any scheduling done by OS</p></li></ul>
</li></ul>

==== Software Interface Layer ====

<ul>
<li><p>Major Layer</p>
{|
!align="center"| ''Program''
|-
|align="center"| '''&lt; File Name, Offest &gt;'''
|-
|align="center"| ''FS/DB''
|-
|align="center"| '''&lt; Partition, #Block &gt;'''
|-
|align="center"| ''Device Driver (Persistent Storage)''
|-
|align="center"| ''Device Driver (Secondary Storage)''
|-
|align="center"| '''&lt; #Disk, #Sector &gt;'''
|-
|align="center"| ''I/O Controller''
|-
|align="center"| '''&lt; Cylinder, Track, Sector &gt;'''
|-
|align="center"| ''Disk Media''
|}
</li>
<li><p>Role</p>
<ul>
<li><p>abstract details from the divice below it</p>
<ul>
<li><p>naming </p></li>
<li><p>address mapping</p></li></ul>
</li>
<li><p>provide abstractions to the divice above it</p>
<ul>
<li><p>caching</p></li>
<li><p>reuest transformation</p></li></ul>
</li></ul>
</li></ul>

==== Disk Interaction ====



=== Disk And FS ===

<blockquote>FS == File System

FFS == Fast File System

LBN == Logical Block Number

Amortization [分期摊销]

oblivious [忽略]

parameterize [参数化]
</blockquote>
==== High-Level Disk Characteristic ====

* Closeness
** reduce seek times by putting related things close to one another
** benifits can be <math display="inline">n^2</math> range
* Amortization
** amortize each positiong delay by grabbing lots of useful data
** benifits can be <math display="inline">n^{10}</math> range

==== FS Key Idea (Role) ====

* be aware of disk characteristics for performance
** allocation algorithms to enhance performance
*** allocating related data close together
** Request scheduling to reduce seek time

==== Allocation Strategy ====

* Goal
** improvment for Inodes on Indirection + Independence
*** +File size grows dynamically
*** +Allocations are independent
*** -Hard to achiece closeness and amortization
* Traditional UNIX FS
** Storage as linear array of blocks
*** each block has a LBN
** Small blocks ( 1k )
*** low bandwidth utilization
*** small max file size ( function of block size)
** Pros
*** simple straightforward implementation 
*** ez to implement and understand
** Cons
*** poor utilization of disk bandwidth (lots of seeking)
**** fragmentaion of an aging file system causes more seeking
**** inodes allocated far from blocks, going back and forth from inodes to data blocks to traverse files/dirs causes more seeking
* BSD FFS : Disk-Aware File System
** Cylinder Group ( aka allocation group )
*** disk partitioned into groups of cylinders
*** data blocks in same file allocated in same grp
*** files in same dir allocated in same grp
*** inodes for files allocated in same grp with file data blocks
*** superblock + bitmap + inode + datablock
* Free space requirenent
** the disk must have free space scattered across cylinder for allocation
*** 10%of the disk is reserved just for this purpose
*** when allocating a large file , break it into large chunks and allocated to different cylinder group instead of full the one group
*** if preferred cylinder group is full allocated from a nearby group
** Fix using larger block (4K)
*** very large files only need two level of indirection
** introduce fragment ( 1k pieces of a block) to fix internal fragmentation
** Pros
** allocation group provides closeness
*** reduce num of long seeks
* Cons
** media failues
*** replicate master block
** device oblivious
*** parameterize according to device characteristics



=== Reliability and Write Optimization ===

<blockquote>FFS == Fast File System

Transaction [交易处理]
</blockquote>
==== Goal ====

* how to guarantee consistency of on-disk storage
* how to handle OS crashes and disk errors
* how to optimize writes

==== Consistency Issue/Crash Recovery ====

* Metadata update must be synchronous operation
** FS op affect multiple metadata blocks
*** write newly allocated inode to disk <math display="inline">\to</math> d_entry name with dir
*** remove a directory name <math display="inline">\to</math> deallocated inode
*** deallocate inode <math display="inline">\to</math> place files' data blocks into free list
** If OS crashes in between any of this op, FS == inconsistent state
* Solution ( overview )
** fsck
*** post-crash recovery process to scan FS structs + restore consistency
**** all data blocks pointed by inodes/indirect blocks must 1 in bitmap
**** all allocated inodes must mapped in some d_entry
**** inode link count must match
** log update
*** enable to roll-back or roll-forward
* Scenairo Example

==== fsck ====

* Definition
** a Unix tool for finding inconsistencies and repairing them
* Limitation
** only care the FS integrity/metadata is consistent
** cannot know when Db is garbage
** too slow
*** disks are very large nowadays scanning all this could take hours
*** even for snall inconsistency must scan whole disk
* Check List
*# Superblock
*#* sanity checks
*#* use another superblock copy if suspected corruption
*# Free block
*#* scan inodes/indirect blocks, build bitmap
*#* inodes/data bitmaps inconsistency <math display="inline">\Rightarrow</math> resolve by trusting inodes
*# Inode state
*#* check inode fields for possible corruption ( ex: must have valid mode )
*#* if cannot fix <math display="inline">\Rightarrow</math> remove inode and update inode bitmap
*# Inode link
*#* verify #links for each inode
*#** traverse directory tree
*#** compute expected #link
*#** fix if needed
*#* if inode discovered but no dir refers to it <math display="inline">\Rightarrow</math> move to &quot;lost+found&quot;
*# Duplicate
*#* check if two different inodes refer to same block
*#* clear one if obviously bad / give each inodes its own copy of block
*# Bad blocks
*#* bad pointers outside of valid range
*#* remove the pointer from the inode or indirecrt block
*# Directory checks
*#* integrity of directory structure
*#** make sure &quot;.&quot; and &quot;..&quot; are the first entries
*#** each inode in a directory entry is allocated
*#** no directory is linked more than once

==== Journaling ====

* aka Write-Ahead-Logging
* Basic Idea
** log the operations you are about to do beforr overwriting structures
** if crash during write<math display="inline">\Rightarrow</math> go back to journal and return the actual write
*** don't need to scan the entire disk
*** can recover data as well
** if crash before journal write finish <math display="inline">\Rightarrow</math> act write not happened
*** nothing is inconsistent
* Jounal Structure
** EXT3 FS Extend EXT2 with journaling capability
*** journal identical with on-disk format
*** can be just another large file
*** can be allocated from anywhere on disk like data blocks
** contain a sequence of transaction
* Transaction Structure
** TxBegin block
*** contain a transaction TID
** blocks with the content to be written
*** physical logging
**** log exact physical content
*** lofiacl logging
**** loh more compact logical representation
** TxEnd block
*** contain a transaction TID
* Journal Trade-off
** Pros
*** recovery is much faster with journaling
**** replay only a few transactions instead of checking the whole disk
*** ensures FS consistency
*** complexity is in the size of the journal instead of the disk
*** metadata journaling is the most commonly used
**** reduce the amount of traffic to the journal
**** provide reasonable consistency guarantee at the same time
** Cons
*** normal ops are slower
**** every update must first write to the journal then do the update
**** writing time is at least doubled
*** journal writing may break sequential writing
**** need to jump back-and-forth betwen write to journal and to real
* Data Jounraling Process
*# write the transaction to the log
*#* option 1 : write each block at a time into Journal 
*#** slow
*#** unsafe if power off before several op before TxEnd same as valid
*#* option 2: split into 2 steps
*#*# Journal Write Step
*#*#* write all except TxEnd each block at a time into Journal 
*#*#* if crash <math display="inline">\Rightarrow</math> skip the pending update
*#*# Journal Commit Step
*#*#* write TxEnd only when step 1 completed into Journal 
*# Checkpoint Step
*#* write the blocks/actual data and metadata to right locations on FS
*#* if crash <math display="inline">\Rightarrow</math> 
*#** on reboot
*#** scan the journal and look for committed transactions
*#** replay these transactions (after this FS is ensured consistent)
*#** called redo loggiing
*# Free Step
*#* mark the transaction free in the journal
* Metadata Journaling Process
** now we only write FS metadata to journal
** option 1: same as data journaling process
*** unsafe 
**** if crash before all data is writen, inode may point to garbage data
** option2: write data before writing metadata to journal
**# Write Data (Wait until it completes)
**#* Data Journal Write Step (optional)
**#* Data Journal Commit Step (optional)
**#* Data Checkpoint Step
**# Metadata Journal Write Step
**# Metadata Journal Commit Step
**# Metadata Checkpoint Step
**# Free Step



=== Log-Structured File System ===

<blockquote>LFS == Log-Structured File System

HDD == Hard Drive

FFS == Fast File System

CR == Checkpoint Region

Locality [地点]

Mitigate [缓和]

Batch [批量]

Obsolete [陈旧的]
</blockquote>
==== Traditional FS ====

* Files lay out with spatial locality in mind
* Changes in place to mitigate seeks
* Avoid fragmenting files 
** keep locality to reads perform well

==== Another Approach ====

* A new approach that prioritizes update performance
* Gist
** instead of overwriting in place, append to log and reclaim obsolete data later (garbage collect)
* Observation + Solution
** Memory increasing
*** don't care about reads
*** most will hit in memory
** assume writes will pose the bigger I/O penalty
*** treat storage as a circular log
* Pros
** write throughout improved/batched into large sequential chunks
** very efficient write
** crash recovery simpler
* Cons
** initial assumption may not hold
*** reads much slower on HDDs?
** less efficient reads
*** more indirection does not solve anthing 
*** but assume most reads hit in memory anyway
** Garbage collection is a tricky problem

==== LFS ====

<ul>
<li><p>Indexed Structure</p>
<ul>
<li><p>write all FS data in a continuous log</p></li>
<li><p>use inode and d_entry from FFS in Segment</p></li>
<li><p>Segment</p>
<ul>
<li><p>superblock</p></li>
<li><p>summary block</p>
<ul>
<li><p>contain pointer to next one</p></li></ul>
</li>
<li><p>inodes</p></li>
<li><p>data blocks</p></li></ul>
</li>
<li><p>if need a fresh segment</p>
<ul>
<li><p>first clean an existing partially-used segment</p></li>
<li><p>garbage collection</p></li></ul>
</li></ul>
</li>
<li><p>Locating Inodes</p>
<ul>
<li><p>Unix FS v.s. LFS</p>
<ul>
<li><p>ez for Unix FS</p>
<ul>
<li><p>array on disk at fixed location</p></li>
<li><p>superblock <math display="inline">\to</math> Inode Block Addr <math display="inline">\to</math> #Inode block <math display="inline">\to</math> #Inode</p></li></ul>
</li>
<li><p>hard for LFS</p>
<ul>
<li><p>updates are sequential</p>
<ul>
<li><p>indoes scattered all over the disk</p></li></ul>
</li>
<li><p>indoes not static keep moving</p></li></ul>
</li></ul>
</li>
<li><p>Imap</p>
<ul>
<li><p>an inode map to find the inodes</p>
<ul>
<li><p>inode number no longer a simple index</p></li></ul>
</li>
<li><p>chunks of imap are placed next to new information</p>
<ul>
<li><p>get updated frequently so not fixed part</p></li>
<li><p>keep seeks and performance</p></li></ul>
</li></ul>
</li>
<li><p>Checkpoint Region</p>
<ul>
<li><p>pointers to the latest pieces of the inode map</p>
<ul>
<li><p>find imap pieces by reading the CR</p></li></ul>
</li></ul>
</li></ul>
</li>
<li><p>Crash Recovery</p>
<ul>
<li><p>Wrting Performance</p>
<ul>
<li><p>buffer writes in segment</p></li>
<li><p>write segment to disk when full/at periodic time intervals</p></li>
<li><p>updates the CR periodically</p></li></ul>
</li>
<li><p>Crash Solution</p>
<ul>
<li><p>Uncommitted Segments</p>
<ul>
<li><p>reconstruct from log after reboot</p></li></ul>
</li>
<li><p>CR</p>
<ul>
<li><p>Keep two CR at either end of the disk</p></li>
<li><p>alternate writes</p></li>
<li><p>update protocol (header/body/last block)</p></li></ul>
</li></ul>
</li></ul>
</li>
<li><p>Garbage Collection</p>
<ul>
<li><p>Cause</p>
<ul>
<li><p>LFS repetedly write latest version of a same file to new location on disk</p></li></ul>
</li>
<li><p>Impact</p>
<ul>
<li><p>Older versions of file ( i.e. Garbage ) are scatterd thoughout the disk</p></li></ul>
</li>
<li><p>Goal</p>
<ul>
<li><p>periodically clean up obslete version to free blks for subsequent write</p></li>
<li><p>cleaning done on a segment-by-segment basis</p>
<ul>
<li><p>the segments are large chunks<math display="inline">\Rightarrow</math> no small holes of free space</p></li></ul>

<p></p></li></ul>
</li></ul>
</li></ul>

=== Disk Failure Fix ===

<blockquote>RAID == Redunant Array of Independent Disk

Thoughput [生产量]

Parity [奇偶校验]
</blockquote>
==== Redundancy ====

* Definition
** have more than one copies of the data
* Goal
** prevent data loss when disk failures ( now journal doen't work )

==== RAID ====

* Reliability Strategy
** data duplicated
*** mirror images
*** redunant full copu
** data spread out across multiple disks with redundancy
*** can recover form a disk failure by reconstructing the data
* Concept
** Redundancy/Mirroring
*** keep multiple copy of the same blk on diff drives in case drive failure
** Parity Info
*** XOR each bit from 2 drives and store checksum on the <math display="inline">3^{rd}</math>drive
* Multiple RAID level
** RAID 0 (2 diff disks)
*** Striping 
**** Files are divied across disks
**** Improve thoughput
**** One drive fails <math display="inline">\Rightarrow</math> the whole volume lost
** RAID 1 (2 same disks)
*** Mirroring 
**** Capacity is half
**** Any drive can serve a read
**** Improved read throughput
**** Write throughput is slower
**** One drive fails <math display="inline">\Rightarrow</math> no data lost
** RAID 5 (3 diff disks + 1 parity disk)
*** Block level striping
*** Distrubuted parity
*** A failed disk can be reconstructed from the rest



=== Solid State Disk ===

<blockquote>SSD == Solid State Disk

HDD == Hard Drive

amplification [扩充]
</blockquote>
==== SSD Info ====

* Definition
** Replace rotating mechanical disks with non-volatile memory
* Type
** Battery-backed RAM
** NAND flash
* Pro
** faster
* Cons
** expensive
** wear-out/ flash-based
* NAND flash storage technology op
** Read
** Write
** Erase!

==== SSD Characteristics ====

* Data cannot be modified &quot;in place&quot;
** no overwrite without erase
* Terminology
** Page
*** unit of read/write op
** Block
*** unit of erase op
** typically one block many pages
* Uniform random access performance
** Disk typically has multi channels so data can be split/stripped across blks
** access time <math display="inline">\uparrow</math>

==== Writing ====

* Problem
** update a bitmap allocation block
*** finde the SSD block containing the target page
*** read all active pages in the SSD block into controller memory
*** update target page with new data in contorller memory
*** erase the SSD block (high voltage to set all bits to 1)
*** write entire SSD block to drive
* Impact
** SSD blocks wear out /limited erase cycle
*** FS blocks are frequently update
* Solution Algorithm
** Wear Leveling
*** always write to new location
*** map &lt; lofical FS #block : &lt; current SSD #block, page location &gt; &gt;
*** old version of logically overwritten pages == stale (garbage collection)
** Garbage Collection
*** reclaiming stale pages and creating empty erased blocks
** RAID 5 (with parity check) striping across I/O channels to multi NAND chips

==== FS And SSDs ====

* Support FS
** EXT4
** btrfs
** XFS
** JFS
** F2FS
* FS no need to take care of wear-leveling
** done internally by SSD
** BUT
*** TRIM op tell SSD which blks are no longer in use
**** otherwise a delete operation doesn't go to disk
* Flsh FS (F2FS/GFFS2) help reduce write amplification 
** especially for small updates on FS metadata
* Other Typical HDD Feature
** Defragmentaion
** Disk scheduling algorithm

==== SSD Reliability ====



=== File System Summary ===

<blockquote>FAT == File Allocation Table

FFS == Fast File System

NTFS == New Technology File System

MFT == Master File Table

VFS == Virtual File System

DS == Data Structure

overhead [磁盘开销]

Sequential Access [顺序存取]

Heuristics [启示]

Locality [局部化]

Consecutive [持续的]

Clustering [集群]
</blockquote>
==== Goal ====

* Efficient directory use to translate file name <math display="inline">\to</math> file number
* Sequential file access performance
* Efficient random access to any file block
* Efficient support for small files
** overhead in terms of space and access time
* Support large files
* Efficient metadata storage and look up
* Crash recovery

==== FS Component ====

* Index structure
** to locate each block of a file
* Free Space Management
* Locality Heuristics
* Crash Recovery

==== FS In Real World ====

<ul>
<li><p>FAT FS</p>
<ul>
<li><p>Link Allocation</p>
<ul>
<li><p>notice that links are in the File Allocation Table, not in the block itself</p></li>
<li><p>blocks allocated in a linked structure table</p></li></ul>
</li>
<li><p>Directory map file name to the first block of file</p></li>
<li><p>FAT store &lt;the linkedlist of blocks of file, free blocks&gt;</p></li>
<li><p>Limitation</p>
<ul>
<li><p>Poor random access</p></li>
<li><p>Poor locality</p></li>
<li><p>Limited file metadata and access control</p></li></ul>
</li></ul>
</li>
<li><p>NTFS</p>
<ul>
<li><p>Each volume is a linear sequence of blocks and a MFT ( 4KB SIZE )</p></li>
<li><p>MFT</p>
<ul>
<li><p>Sequence of 1KB records</p></li>
<li><p>one or more record per file/dir</p>
<ul>
<li><p>similar to inodes but more flexible</p></li></ul>
</li>
<li><p>record is a sequence of var_length &lt;attribute header, value &gt;</p></li>
<li><p>long attributes can be stored externally using a ptr kept in MFT record</p></li></ul>
</li>
<li><p>Extent/Run allocation for consecutive blocks</p></li>
<li><p>Metadata/Attributes</p>
<ul>
<li><p>key-value pairs</p></li>
<li><p>significant flexibility</p>
<ul>
<li><p>allow implement extra feature like compression and diff file type</p></li></ul>
</li></ul>
</li></ul>
</li>
<li><p>VFS</p>
<ul>
<li><p>an abstract file system interface</p>
<ul>
<li><p>Separate abstraction and collections of file from speci implementation</p></li>
<li><p>sys calls implemented in terms of operations on the abstract file sys</p>
<ul>
<li><p>vfs_open/close</p></li></ul>
</li></ul>
</li>
<li><p>abstraction layer is for OS itself</p>
<ul>
<li><p>user-level programmers interacts with the file sys through the sys calls</p></li></ul>
</li></ul>
</li>
<li><p>EXT3</p>
<ul>
<li><p>Pros</p>
<ul>
<li><p>recoverability</p></li>
<li><p>recoverable when significant data corruption ( other tree-based not )</p></li></ul>
</li>
<li><p>Cons</p>
<ul>
<li><p>lack modern FS feature like extent</p></li>
<li><p>metadata is in fixed well-known location and DS have redundancy</p></li></ul>
</li></ul>
</li>
<li><p>FFS</p>
<ul>
<li><p>Inode</p>
<ul>
<li><p>store disk block index</p></li>
<li><p>store metadata</p></li></ul>
</li>
<li><p>D_entry</p>
<ul>
<li><p>store file name and inode number</p></li></ul>
</li>
<li><p>Free Space Management</p>
<ul>
<li><p>bitmap</p></li></ul>
</li>
<li><p>Read Performance</p>
<ul>
<li><p>locate related blocks in same cylinder group</p></li>
<li><p>locate inodes close to data blocks</p></li></ul>
</li>
<li><p>Write Performance</p>
<ul>
<li><p>block reallocation</p>
<ul>
<li><p>reduce fragmentation</p></li>
<li><p>control aging</p></li></ul>
</li>
<li><p>soft update</p>
<ul>
<li><p>alternatice to journaling</p></li>
<li><p>ensures consistency without limiting performance</p></li></ul>
</li>
<li><p>background fsck</p>
<ul>
<li><p>for the rest of failure issues</p></li></ul>
</li></ul>
</li>
<li><p>Performance Observation</p>
<ul>
<li><p>optimized for disk block clustering</p></li>
<li><p>using properties of the disk to inform file system layout</p></li>
<li><p>when memory is large enough</p>
<ul>
<li><p>most reads that go to the disk ar the first read of the file</p></li>
<li><p>subsequent reads are satisfied in memeory by file buffer cache</p></li>
<li><p>writes are not well-clustered ?</p></li></ul>
</li></ul>
</li></ul>
</li>
<li><p>Popular FS List</p>
<ul>
<li><p>Windows</p>
<ul>
<li><p>FAT32</p></li>
<li><p>NTFS</p></li></ul>
</li>
<li><p>MAC OS X</p>
<ul>
<li><p>HFS+</p></li></ul>
</li>
<li><p>BSD/Solaris</p>
<ul>
<li><p>UFS</p></li>
<li><p>ZFS</p></li></ul>
</li>
<li><p>Linux</p>
<ul>
<li><p>EXT2/3/4</p>
<ul>
<li><p>EXT2 borrowed heavily from FFS</p></li>
<li><p>updated on reliadbility and write optimization</p></li></ul>
</li>
<li><p>ReiserFS</p></li>
<li><p>XFS/JFS</p></li>
<li><p>btrfs</p></li>
<li><p>zfs</p></li></ul>
</li></ul>
</li>
<li><p>FS Comparison</p>
{|
! 
!align="center"| FAT
!align="center"| FFS
!align="center"| NTFS
|-
| Index Structure
|align="center"| LinkedList
|align="center"| Tree( inodes )
|align="center"| Tree( Extent )
|-
| Index Structure Granularity
|align="center"| Block
|align="center"| Block
|align="center"| Extent
|-
| Free Space Management
|align="center"| FAT array
|align="center"| Bitmap
|align="center"| Bitmap
|-
| Locality Heuristics
|align="center"| Defragmentation
|align="center"| Block group reserved space
|align="center"| Best fit defragmentation
|}

<p> </p></li></ul>









